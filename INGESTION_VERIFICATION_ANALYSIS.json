{
  "analysis_timestamp": "2026-01-23",
  "verification_summary": "Deep code inspection reveals multiple critical bottlenecks and several incorrect assumptions in the initial analysis",

  "verified_bottlenecks": [
    {
      "file": "backend/app/ingestion/application/orchestrator.py",
      "lines": "318-356",
      "actual_code_snippet": "for chunk_idx, chunk in enumerate(chunks):\n    # Check for shutdown request\n    if self.is_shutdown_requested():\n        ...\n    # Gate check\n    if not await self._check_gate(ctx.job_id, ctx.kb_id, indexer):\n        ...\n    task = ProcessingTask(...)\n    result = await self._process_chunk_with_retry(task, chunk, embedder, indexer)\n    if result['skipped']:\n        ctx.counters['chunks_skipped'] += 1\n    elif result['success']:\n        ctx.counters['chunks_processed'] += 1\n    else:\n        ctx.counters['chunks_error'] += 1",
      "confirmed_issue": "SEQUENTIAL CHUNK PROCESSING - Chunks are processed ONE AT A TIME in a for-loop with await. No parallelism whatsoever.",
      "severity": "critical",
      "evidence": "Line 318: 'for chunk_idx, chunk in enumerate(chunks)' with 'await' inside means each chunk waits for the previous one to complete. With 1000 chunks and 200ms per embed+index, this is 200 seconds minimum."
    },
    {
      "file": "backend/app/ingestion/domain/embedding/embedder.py",
      "lines": "53-94",
      "actual_code_snippet": "async def embed(self, chunk: Chunk) -> EmbeddingResult:\n    if not chunk.text or not chunk.text.strip():\n        raise ValueError('Cannot embed empty chunk text')\n    try:\n        logger.info(f'→ Calling OpenAI API: model={self.model_name}, chunk={chunk.content_hash[:8]}, size={len(chunk.text)} chars')\n        # Generate embedding (sync call, but fast enough)\n        # LlamaIndex OpenAIEmbedding handles retries internally\n        vector = self.embedding_client.get_text_embedding(chunk.text)\n        logger.info(f'✓ OpenAI API response: {len(vector)} dimensions, chunk={chunk.content_hash[:8]}')\n        if not vector:\n            raise RuntimeError('Embedding generation returned empty vector')\n        return EmbeddingResult(...)",
      "confirmed_issue": "SYNCHRONOUS BLOCKING EMBEDDING CALL - Uses get_text_embedding (sync) instead of aget_text_embedding (async), despite being in an async function",
      "severity": "critical",
      "evidence": "Line 78: 'vector = self.embedding_client.get_text_embedding(chunk.text)' - This is a synchronous call that blocks the event loop during the 100-200ms OpenAI API call. The comment '(sync call, but fast enough)' is misleading - 200ms x 1000 chunks = 200 seconds of blocked event loop time."
    },
    {
      "file": "backend/app/ingestion/application/orchestrator.py",
      "lines": "423-437",
      "actual_code_snippet": "exists = await asyncio.to_thread(indexer.exists, task.kb_id, chunk.content_hash)\nif exists:\n    logger.debug(f'Chunk {chunk.content_hash[:8]} already indexed, skipping')\n    return {'success': True, 'skipped': True}\n# Try embed + index with retry\nattempt = 0\nwhile True:\n    attempt += 1\n    try:\n        # Embed\n        embedding = await embedder.embed(chunk)\n        # Index\n        await asyncio.to_thread(indexer.index, task.kb_id, embedding)",
      "confirmed_issue": "IDEMPOTENCY CHECK PER CHUNK - Every chunk calls indexer.exists() which loads the entire index from disk and checks hash cache",
      "severity": "high",
      "evidence": "Line 423: 'exists = await asyncio.to_thread(indexer.exists, ...)' runs in thread pool for EVERY chunk. With 1000 chunks, this means 1000 disk reads/index loads. The indexer._load_index() reads docstore.json and rebuilds the hash cache each time."
    },
    {
      "file": "backend/app/ingestion/domain/indexing/indexer.py",
      "lines": "120-147",
      "actual_code_snippet": "def index(self, kb_id: str, embedding_result: EmbeddingResult) -> None:\n    if kb_id != self.kb_id:\n        raise ValueError(f'KB ID mismatch: expected {self.kb_id}, got {kb_id}')\n    if not embedding_result.vector:\n        raise ValueError('Embedding result has no vector')\n    # Create LlamaIndex Document with embedding\n    doc = Document(text=embedding_result.text, metadata=embedding_result.metadata, id_=embedding_result.content_hash, embedding=embedding_result.vector)\n    # Load or create index\n    index = self._load_index()\n    if index is None:\n        os.makedirs(self.storage_dir, exist_ok=True)\n        index = VectorStoreIndex([doc])\n        self._index = index\n        logger.info(f'Created new index for KB {kb_id}')\n    else:\n        index.insert(doc)\n    # Update cache\n    self._indexed_hashes.add(embedding_result.content_hash)\n    # Persist immediately (atomic write)\n    index.storage_context.persist(persist_dir=self.storage_dir)",
      "confirmed_issue": "DISK PERSIST PER CHUNK - Every single chunk triggers index.storage_context.persist() which writes all index files to disk",
      "severity": "critical",
      "evidence": "Line 147: 'index.storage_context.persist(persist_dir=self.storage_dir)' is called after EVERY chunk.index() call. This writes docstore.json, vector_store.json, and index_store.json to disk synchronously. With 1000 chunks and ~50-100ms per persist, this adds 50-100 seconds of pure disk I/O."
    },
    {
      "file": "backend/app/ingestion/domain/sources/website/crawler.py",
      "lines": "140-162",
      "actual_code_snippet": "while to_visit and len(visited) < max_pages:\n    url = self._get_next_valid_url(to_visit, visited)\n    if not url:\n        break\n    last_id += 1\n    visited.add(url)\n    doc, final_url, links = self._fetch_and_process_url(url, last_id)\n    if not doc and not links:\n        failed_count += 1\n        time.sleep(RATE_LIMIT_DELAY)\n        continue\n    self._handle_redirect(url, final_url, visited)\n    if doc:\n        current_batch.append(doc)\n        if len(current_batch) >= batch_size:\n            yield current_batch\n            current_batch = []\n    self._queue_new_links(links, visited, to_visit)\n    self._log_progress(len(visited), len(to_visit))\n    time.sleep(RATE_LIMIT_DELAY)",
      "confirmed_issue": "BLOCKING SYNCHRONOUS CRAWLER WITH SLEEP DELAYS - Crawler uses synchronous requests.get() and time.sleep(0.5s) between every page fetch",
      "severity": "high",
      "evidence": "Lines 148, 161: 'time.sleep(RATE_LIMIT_DELAY)' where RATE_LIMIT_DELAY=0.5 seconds. Line 272: 'requests.get()' is a blocking synchronous HTTP call. For 100 pages, that's 50 seconds of artificial delay PLUS blocking HTTP I/O. This runs in asyncio.to_thread but still serializes crawler work."
    },
    {
      "file": "backend/app/ingestion/domain/sources/website/crawler.py",
      "lines": "270-298",
      "actual_code_snippet": "def _fetch_html_with_redirect(self, url: str) -> tuple[str | None, str | None]:\n    for attempt in range(self.max_retries):\n        try:\n            response = requests.get(url, timeout=self.timeout, headers=self.headers, allow_redirects=True)\n            response.raise_for_status()\n            final_url = response.url if response.history else url\n            return response.text, final_url\n        except requests.exceptions.HTTPError as e:\n            status = e.response.status_code\n            if HTTP_BAD_REQUEST <= status < HTTP_INTERNAL_ERROR:\n                logger.warning(f'  ✗ Client error {status}: {url}')\n                return None, None\n            self._handle_retry(attempt, str(e))\n        except requests.exceptions.RequestException as e:\n            self._handle_retry(attempt, str(e))\n    return None, None",
      "confirmed_issue": "SYNCHRONOUS BLOCKING HTTP REQUESTS - Uses requests library (blocking) instead of aiohttp/httpx async client",
      "severity": "medium",
      "evidence": "Line 272: 'requests.get()' blocks thread during network I/O. With retry logic and RETRY_DELAY=2.0 seconds (line 298), failed pages add significant latency. This prevents any parallelism during page fetching."
    },
    {
      "file": "backend/app/ingestion/infrastructure/job_repository.py",
      "lines": "56-67",
      "actual_code_snippet": "with get_session() as session:\n    session.add(job)\n    session.flush()\n    job_id = job.id\nself.initialize_phase_statuses(job_id)\nreturn cast(str, job_id)",
      "confirmed_issue": "DATABASE COMMITS PER OPERATION - Context manager auto-commits after every repository call",
      "severity": "medium",
      "evidence": "ingestion_database.py line 62: 'session.commit()' is called in the context manager __exit__. Every update_job(), update_heartbeat(), set_job_status() call opens a new session and commits. With status updates per batch and heartbeat updates, this adds SQLite lock contention."
    },
    {
      "file": "backend/app/ingestion/application/orchestrator.py",
      "lines": "278-296",
      "actual_code_snippet": "async def _run_load_phase(self, ctx: PipelineBatchContext, batch: list[Any]) -> None:\n    await asyncio.to_thread(save_documents_to_disk, ctx.kb_id, batch)\n    with contextlib.suppress(Exception):\n        self.phase_repo.update_progress(ctx.job_id, 'loading', items_processed=ctx.counters['docs_seen'])\n\nasync def _run_chunk_phase(self, ctx: PipelineBatchContext, batch: list[Any], chunker: Any) -> list[Any]:\n    chunks = await asyncio.to_thread(chunk_documents_to_chunks, batch, chunker, ctx.kb_id)\n    ctx.counters['docs_seen'] += len(batch)\n    ctx.counters['chunks_seen'] += len(chunks)\n    with contextlib.suppress(Exception):\n        self.phase_repo.update_progress(ctx.job_id, 'loading', items_processed=ctx.counters['docs_seen'])\n        self.phase_repo.start_phase(ctx.job_id, 'chunking')\n        ctx.phases_started['chunking'] = True\n        self.phase_repo.update_progress(ctx.job_id, 'chunking', items_processed=ctx.counters['chunks_seen'])",
      "confirmed_issue": "BLOCKING OPERATIONS WRAPPED IN TO_THREAD - Loading and chunking use asyncio.to_thread but compete for the same threadpool as embedding/indexing",
      "severity": "low",
      "evidence": "Lines 278, 288: save_documents_to_disk and chunk_documents_to_chunks run in asyncio's default threadpool (max_workers=12 on this system). When combined with sequential chunk processing, the threadpool doesn't help much. The real issue is the sequential for-loop, not the threadpool itself."
    }
  ],

  "challenged_assumptions": [
    {
      "original_assumption": "embedding_client.get_text_embedding is a synchronous blocking call",
      "verification_result": "confirmed",
      "actual_findings": "embedder.py line 78 uses self.embedding_client.get_text_embedding(chunk.text) which is synchronous. HOWEVER, verification reveals that OpenAIEmbedding from llama-index DOES have async methods: aget_text_embedding and aget_text_embedding_batch. The current code is using the wrong method.",
      "corrected_analysis": "The bottleneck is real but fixable. OpenAIEmbedding provides 'aget_text_embedding()' for async operation. Current code comment says '(sync call, but fast enough)' which is factually incorrect - 200ms x 1000 = 200 seconds."
    },
    {
      "original_assumption": "Per-chunk persistence is happening",
      "verification_result": "confirmed",
      "actual_findings": "indexer.py line 147: 'index.storage_context.persist(persist_dir=self.storage_dir)' is called IMMEDIATELY after every index.insert(doc) operation. This is NOT batch persistence - it's per-chunk persistence.",
      "corrected_analysis": "This is confirmed as a critical bottleneck. Each persist() writes multiple JSON files (docstore.json, vector_store.json, index_store.json) to disk synchronously. This should only happen once per batch or at the end of ingestion."
    },
    {
      "original_assumption": "Chunks processed sequentially in for-loop",
      "verification_result": "confirmed",
      "actual_findings": "orchestrator.py line 318: 'for chunk_idx, chunk in enumerate(chunks):' with 'await self._process_chunk_with_retry(task, chunk, embedder, indexer)' inside. This is a sequential await pattern - no asyncio.gather, no asyncio.create_task, no concurrent.futures.",
      "corrected_analysis": "100% confirmed. There is ZERO parallelism in chunk processing. Each chunk waits for the previous one to complete: gate check → idempotency check → embed → index → persist."
    },
    {
      "original_assumption": "asyncio.to_thread causes threadpool queuing issues",
      "verification_result": "partial",
      "actual_findings": "Python 3.10 with default ThreadPoolExecutor has max_workers=(os.cpu_count() or 1) = 12 threads on this system. asyncio.to_thread() is used for indexer.exists() and indexer.index() but since chunks are processed sequentially, there's no actual threadpool contention - only one chunk is processed at a time.",
      "corrected_analysis": "The threadpool itself is not the bottleneck. The bottleneck is the sequential for-loop that prevents any concurrency. asyncio.to_thread would only cause queuing if we had parallel chunk processing (which we don't). The real issue is: (1) sequential processing, (2) sync embedding call blocking event loop, (3) per-chunk disk persist."
    },
    {
      "original_assumption": "Loader/crawler yields batches efficiently",
      "verification_result": "refuted",
      "actual_findings": "crawler.py line 140-161: The crawler IS a generator that yields batches, BUT it uses synchronous requests.get() blocking calls with time.sleep(0.5) between EVERY page fetch. The crawler itself is run in asyncio.to_thread (line 209) which prevents event loop blocking, but the crawler still processes pages sequentially with artificial delays.",
      "corrected_analysis": "The crawler is not as efficient as assumed. It's a blocking generator with synchronous HTTP and rate-limit sleeps. For 100 pages: (1) 100 x 0.5s = 50s of artificial rate limiting, (2) plus HTTP fetch time, (3) all serialized. The crawler should use async HTTP (aiohttp/httpx) and remove/reduce artificial delays."
    }
  ],

  "missed_bottlenecks": [
    {
      "category": "network",
      "description": "OpenAI API has NATIVE ASYNC SUPPORT that's not being used - aget_text_embedding() and aget_text_embedding_batch() are available",
      "location": "backend/app/ingestion/domain/embedding/embedder.py line 78",
      "impact_estimate": "By using get_text_embedding (sync) instead of aget_text_embedding (async), the code blocks the event loop during every 100-200ms OpenAI API call. This prevents any other async work from running and eliminates the possibility of concurrent embeddings."
    },
    {
      "category": "io",
      "description": "LlamaIndex persist() writes multiple files per chunk - not just one file",
      "location": "backend/app/ingestion/domain/indexing/indexer.py line 147",
      "impact_estimate": "storage_context.persist() writes at least 3 files: docstore.json (contains all document text), vector_store.json (embeddings), and index_store.json (metadata). Each persist reads the existing files, updates them in memory, and writes them back. With 1000 chunks, that's 3000+ file write operations. On a slow disk or networked storage, this could be 100-200 seconds of I/O."
    },
    {
      "category": "memory",
      "description": "Indexer._load_index() rebuilds hash cache from docstore on every exists() check",
      "location": "backend/app/ingestion/domain/indexing/indexer.py lines 55-70",
      "impact_estimate": "Every indexer.exists() call potentially triggers _load_index() which reads docstore.json and iterates through all documents to build _indexed_hashes set. With 1000 chunks and growing index size, early chunks are fast but later chunks parse a large JSON file. This is O(n) per chunk = O(n²) overall."
    },
    {
      "category": "database",
      "description": "SQLite auto-commit per repository operation creates lock contention",
      "location": "backend/app/ingestion/ingestion_database.py line 62 + all repository methods",
      "impact_estimate": "Every phase_repo.update_progress(), repo.update_heartbeat(), repo.update_job() opens a new session, performs query, and commits. With batches and per-chunk updates, this could be hundreds of commits. SQLite default isolation requires exclusive locks for writes, causing serialization. Not huge but measurable - probably 5-10 seconds over 1000 chunks."
    },
    {
      "category": "network",
      "description": "Website crawler uses blocking requests library instead of async HTTP client",
      "location": "backend/app/ingestion/domain/sources/website/crawler.py line 272",
      "impact_estimate": "Using requests.get() means each page fetch blocks a thread completely during network I/O (could be 100-500ms per page). An async HTTP client (aiohttp, httpx) would allow the thread to process other work during I/O wait. With 100 pages, this adds 10-50 seconds of pure wait time."
    },
    {
      "category": "other",
      "description": "Artificial rate limiting with time.sleep(0.5) after every page fetch",
      "location": "backend/app/ingestion/domain/sources/website/crawler.py lines 148, 161",
      "impact_estimate": "RATE_LIMIT_DELAY = 0.5 seconds is hard-coded and applied after EVERY page, successful or failed. For 100 pages, this is 50 seconds of artificial delay that serves no purpose for most websites. This should be configurable and only applied to rate-limited domains."
    },
    {
      "category": "other",
      "description": "Excessive logging with multiple logger.info() calls per chunk",
      "location": "backend/app/ingestion/domain/embedding/embedder.py lines 69, 80",
      "impact_estimate": "Two logger.info() calls per chunk (→ Calling OpenAI API, ✓ OpenAI API response) means 2000 log writes for 1000 chunks. If logs are written to disk synchronously, this could add 1-5 seconds. Not critical but wasteful - should be logger.debug()."
    }
  ],

  "performance_math": {
    "theoretical_minimum_seconds": {
      "embedding_api_calls": "1000 chunks × 200ms avg = 200 seconds if sequential, 25 seconds if 8 concurrent",
      "disk_persist": "1000 chunks × 80ms avg (3 files write/read) = 80 seconds if per-chunk, 0.5 seconds if batched",
      "idempotency_checks": "1000 chunks × 20ms avg (load index + hash check) = 20 seconds, could be reduced to 0 with proper caching",
      "crawler_fetch": "100 pages × (500ms fetch + 500ms rate limit) = 100 seconds sequential, 15-20 seconds if 5-8 concurrent",
      "chunking": "100 docs × 50ms = 5 seconds (minor)",
      "database_updates": "~100 commits × 50ms = 5 seconds (minor)",
      "total_sequential": "200 + 80 + 20 + 100 + 5 + 5 = 410 seconds (~7 minutes)",
      "total_optimized": "25 + 0.5 + 0 + 20 + 5 + 5 = 55.5 seconds (~1 minute)"
    },
    "observed_behavior": "User reports 'slowness' without specific timing. Based on code analysis, expected time for 100 pages with 1000 total chunks would be 5-10 minutes with current implementation.",
    "discrepancy_factor": "Current implementation is ~7-8x slower than optimal. The biggest wins are: (1) parallel chunk processing (8x speedup on embeddings), (2) batch persistence (160x reduction in disk I/O), (3) async embeddings + crawler (2-3x speedup)."
  },

  "revised_recommendations": [
    {
      "priority": 10,
      "title": "Use async embedding with batch parallel processing",
      "rationale": "Embedder has aget_text_embedding() available but isn't using it. Combined with asyncio.gather() for concurrent chunk processing, this gives 8-10x speedup on embeddings alone.",
      "code_change": {
        "file": "backend/app/ingestion/domain/embedding/embedder.py",
        "before": "async def embed(self, chunk: Chunk) -> EmbeddingResult:\n    ...\n    vector = self.embedding_client.get_text_embedding(chunk.text)\n    ...",
        "after": "async def embed(self, chunk: Chunk) -> EmbeddingResult:\n    ...\n    vector = await self.embedding_client.aget_text_embedding(chunk.text)\n    ...",
        "expected_impact": "Eliminates event loop blocking. Enables true concurrency when combined with parallel processing in orchestrator. Reduces embedding time from 200s to 25-30s for 1000 chunks."
      }
    },
    {
      "priority": 10,
      "title": "Process chunks in parallel batches with asyncio.gather()",
      "rationale": "Current sequential for-loop is the primary bottleneck. Using asyncio.gather() with concurrency limit allows 8-12 chunks to be processed simultaneously.",
      "code_change": {
        "file": "backend/app/ingestion/application/orchestrator.py",
        "before": "for chunk_idx, chunk in enumerate(chunks):\n    if self.is_shutdown_requested():\n        ...\n    if not await self._check_gate(ctx.job_id, ctx.kb_id, indexer):\n        ...\n    task = ProcessingTask(...)\n    result = await self._process_chunk_with_retry(task, chunk, embedder, indexer)\n    if result['skipped']:\n        ctx.counters['chunks_skipped'] += 1\n    elif result['success']:\n        ctx.counters['chunks_processed'] += 1\n    else:\n        ctx.counters['chunks_error'] += 1",
        "after": "# Process chunks in parallel batches (e.g., 10 at a time)\nCONCURRENCY_LIMIT = 10\nfor i in range(0, len(chunks), CONCURRENCY_LIMIT):\n    if self.is_shutdown_requested():\n        ...\n    if not await self._check_gate(ctx.job_id, ctx.kb_id, indexer):\n        ...\n    batch_chunks = chunks[i:i+CONCURRENCY_LIMIT]\n    tasks = [\n        self._process_chunk_with_retry(\n            ProcessingTask(...), \n            chunk, \n            embedder, \n            indexer\n        ) \n        for chunk_idx, chunk in enumerate(batch_chunks, start=i)\n    ]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    for result in results:\n        if isinstance(result, Exception):\n            ctx.counters['chunks_error'] += 1\n        elif result.get('skipped'):\n            ctx.counters['chunks_skipped'] += 1\n        elif result.get('success'):\n            ctx.counters['chunks_processed'] += 1\n        else:\n            ctx.counters['chunks_error'] += 1",
        "expected_impact": "8-10x speedup on chunk processing. With 10 concurrent chunks and async embeddings, 1000 chunks go from 200+ seconds to 25-30 seconds."
      }
    },
    {
      "priority": 9,
      "title": "Batch index persistence - only persist after batch or every N chunks",
      "rationale": "Persisting after every chunk is catastrophically slow. Batching persistence reduces disk I/O from 1000 operations to 10-50 operations.",
      "code_change": {
        "file": "backend/app/ingestion/domain/indexing/indexer.py",
        "before": "def index(self, kb_id: str, embedding_result: EmbeddingResult) -> None:\n    ...\n    # Update cache\n    self._indexed_hashes.add(embedding_result.content_hash)\n    # Persist immediately (atomic write)\n    index.storage_context.persist(persist_dir=self.storage_dir)\n    logger.debug(f'Indexed chunk {embedding_result.content_hash[:8]}')",
        "after": "def index(self, kb_id: str, embedding_result: EmbeddingResult, auto_persist: bool = False) -> None:\n    ...\n    # Update cache\n    self._indexed_hashes.add(embedding_result.content_hash)\n    # Only persist if explicitly requested (for batching)\n    if auto_persist:\n        index.storage_context.persist(persist_dir=self.storage_dir)\n        logger.debug(f'Indexed and persisted chunk {embedding_result.content_hash[:8]}')\n    else:\n        logger.debug(f'Indexed chunk {embedding_result.content_hash[:8]} (persist deferred)')\n\ndef persist(self) -> None:\n    \"\"\"Manually persist the index (call after batch of inserts).\"\"\"\n    if self._index:\n        self._index.storage_context.persist(persist_dir=self.storage_dir)\n        logger.info(f'Persisted index with {len(self._indexed_hashes)} documents')",
        "expected_impact": "Reduces disk I/O from ~1000 persist operations to ~10-50 (one per batch). Saves 60-70 seconds on 1000 chunks. Critical for performance."
      }
    },
    {
      "priority": 8,
      "title": "Cache indexed hashes at orchestrator level to avoid repeated index loads",
      "rationale": "indexer.exists() calls _load_index() repeatedly, reading and parsing docstore.json. Build hash cache once and reuse.",
      "code_change": {
        "file": "backend/app/ingestion/application/orchestrator.py",
        "before": "async def _process_chunk_with_retry(self, task: ProcessingTask, chunk: Any, embedder: Embedder, indexer: Indexer) -> dict[str, Any]:\n    # Check idempotency first\n    exists = await asyncio.to_thread(indexer.exists, task.kb_id, chunk.content_hash)\n    if exists:\n        logger.debug(f'Chunk {chunk.content_hash[:8]} already indexed, skipping')\n        return {'success': True, 'skipped': True}",
        "after": "async def _run_embed_and_index_phase(self, ctx: PipelineBatchContext, chunks: list[Any], embedder: Embedder, indexer: Indexer) -> bool:\n    # Load index ONCE and build hash cache\n    await asyncio.to_thread(indexer._load_index)\n    indexed_hashes = indexer._indexed_hashes  # Reference to cache\n    ...\n\nasync def _process_chunk_with_retry(self, task: ProcessingTask, chunk: Any, embedder: Embedder, indexer: Indexer, indexed_hashes: set[str]) -> dict[str, Any]:\n    # Check idempotency from cache (no I/O)\n    if chunk.content_hash in indexed_hashes:\n        logger.debug(f'Chunk {chunk.content_hash[:8]} already indexed, skipping')\n        return {'success': True, 'skipped': True}",
        "expected_impact": "Eliminates 1000 disk reads for idempotency checks. Saves 15-20 seconds. Reduces from O(n²) to O(n) complexity."
      }
    },
    {
      "priority": 7,
      "title": "Use async HTTP client (httpx/aiohttp) in crawler",
      "rationale": "requests.get() is blocking and prevents concurrent page fetching. Async HTTP allows parallel crawling with asyncio.gather().",
      "code_change": {
        "file": "backend/app/ingestion/domain/sources/website/crawler.py",
        "before": "import requests\n...\nresponse = requests.get(url, timeout=self.timeout, headers=self.headers, allow_redirects=True)",
        "after": "import httpx\n...\nasync with httpx.AsyncClient() as client:\n    response = await client.get(url, timeout=self.timeout, headers=self.headers, follow_redirects=True)",
        "expected_impact": "Enables concurrent page fetching. With 5-8 concurrent fetches, 100 pages go from 100s to 15-20s. Requires making crawler async and updating orchestrator."
      }
    },
    {
      "priority": 6,
      "title": "Remove or make configurable the artificial rate limit delay",
      "rationale": "time.sleep(0.5) after every page adds 50 seconds for 100 pages. Most websites don't need this aggressive rate limiting.",
      "code_change": {
        "file": "backend/app/ingestion/domain/sources/website/crawler.py",
        "before": "RATE_LIMIT_DELAY = 0.5\n...\ntime.sleep(RATE_LIMIT_DELAY)",
        "after": "RATE_LIMIT_DELAY = 0.1  # Reduced from 0.5 or make configurable\n...\nif self.rate_limit_delay > 0:\n    await asyncio.sleep(self.rate_limit_delay)  # Use async sleep if crawler is async",
        "expected_impact": "Saves 40 seconds on 100 pages (0.5s → 0.1s per page). Low risk, immediate win."
      }
    },
    {
      "priority": 5,
      "title": "Batch database updates instead of per-operation commits",
      "rationale": "SQLite auto-commit per operation creates lock contention. Batch updates reduce commit overhead.",
      "code_change": {
        "file": "backend/app/ingestion/ingestion_database.py",
        "before": "@contextmanager\ndef get_session() -> Iterator[Session]:\n    session: Session = SessionLocal()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()",
        "after": "@contextmanager\ndef get_session(auto_commit: bool = True) -> Iterator[Session]:\n    session: Session = SessionLocal()\n    try:\n        yield session\n        if auto_commit:\n            session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\n# Then in orchestrator, batch counter updates per batch instead of per chunk",
        "expected_impact": "Reduces SQLite commits from ~1000 to ~100. Saves 3-5 seconds. Minor but helps."
      }
    },
    {
      "priority": 4,
      "title": "Use logger.debug() instead of logger.info() for per-chunk operations",
      "rationale": "2 log statements per chunk = 2000 log writes for 1000 chunks. This is excessive and slows down production.",
      "code_change": {
        "file": "backend/app/ingestion/domain/embedding/embedder.py",
        "before": "logger.info(f'→ Calling OpenAI API: model={self.model_name}, chunk={chunk.content_hash[:8]}, size={len(chunk.text)} chars')\n...\nlogger.info(f'✓ OpenAI API response: {len(vector)} dimensions, chunk={chunk.content_hash[:8]}')",
        "after": "logger.debug(f'→ Calling OpenAI API: model={self.model_name}, chunk={chunk.content_hash[:8]}, size={len(chunk.text)} chars')\n...\nlogger.debug(f'✓ OpenAI API response: {len(vector)} dimensions, chunk={chunk.content_hash[:8]}')",
        "expected_impact": "Reduces log I/O noise. Saves 1-3 seconds on 1000 chunks. Improves log readability."
      }
    },
    {
      "priority": 3,
      "title": "Use OpenAI batch embedding API if available",
      "rationale": "OpenAIEmbedding has aget_text_embedding_batch() which can embed multiple chunks in one API call, reducing network overhead.",
      "code_change": {
        "file": "backend/app/ingestion/domain/embedding/embedder.py",
        "before": "async def embed(self, chunk: Chunk) -> EmbeddingResult:\n    vector = await self.embedding_client.aget_text_embedding(chunk.text)",
        "after": "async def embed_batch(self, chunks: list[Chunk]) -> list[EmbeddingResult]:\n    texts = [chunk.text for chunk in chunks]\n    vectors = await self.embedding_client.aget_text_embedding_batch(texts)\n    return [\n        EmbeddingResult(vector=v, content_hash=c.content_hash, text=c.text, metadata=c.metadata)\n        for v, c in zip(vectors, chunks)\n    ]",
        "expected_impact": "Further reduces API overhead by batching. May reduce embedding time by another 20-30% (from 25s to 18-20s for 1000 chunks)."
      }
    },
    {
      "priority": 2,
      "title": "Add retry with exponential backoff at orchestrator level instead of per-method",
      "rationale": "Current retry logic is scattered across methods. Centralizing with tenacity or similar library provides better observability and control.",
      "code_change": {
        "file": "backend/app/ingestion/application/orchestrator.py",
        "before": "# Retry logic embedded in _process_chunk_with_retry with manual while loop",
        "after": "from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\nasync def _embed_and_index_chunk(self, chunk, embedder, indexer, kb_id):\n    embedding = await embedder.embed(chunk)\n    await asyncio.to_thread(indexer.index, kb_id, embedding, auto_persist=False)\n    return {'success': True, 'skipped': False}",
        "expected_impact": "Cleaner code, better retry behavior. No direct performance impact but improves maintainability."
      }
    }
  ],

  "confidence_level": "high - All findings are based on actual code inspection with line numbers, verified API documentation (OpenAIEmbedding async methods), and measured thread pool capacity. The math is conservative (assumes 200ms per embedding which is on the high side). The recommendations are ordered by impact and feasibility."
}
